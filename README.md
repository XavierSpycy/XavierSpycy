![](assets/Bottom_up.svg)

![](assets/chris-ried-python-crop.jpg)

Photo by <a href="https://unsplash.com/@cdr6934?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Chris Ried</a> on <a href="https://unsplash.com/photos/ieic5Tq8YMk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>

<a><img src="https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg" align="right" height="48" width="48" ></a>

[![Typing SVG](https://readme-typing-svg.herokuapp.com?color=%2336BCF7&center=true&vCenter=true&width=600&lines=ðŸ‘‹++Hi~,+I+am+Jiarui+Xu~;+Welcome+to+my+profile!;Master's+degree+of+Data+Science.;Machine+learning+specialization.+;Python+programming+enthusiast.+;Patience+and+persistence.)](https://git.io/typing-svg)

```mermaid
graph TD;
    MyDomainKnowledge[My Domain Knowledge]-->MachineLearning[Machine Learning];
    MachineLearning[Machine Learning]-->StatisticalLearning[Statistical Learning];
    MachineLearning[Machine Learning]-->DeepLearning[Deep Learning];
    MyDomainKnowledge[My Domain Knowledge]-->SoftwareDevelopment[Software Development];
    DeepLearning[Deep Learning]-->ComputerVision[Computer Vision];
    DeepLearning[Deep Learning]-->NaturalLanguageProcessing[Natural Language Processing];
    DeepLearning[Deep Learning]-->Multimodality;
   ```

| Properties|Skills|
|-|-|
| **Domain Knownledge**| ![](https://img.shields.io/badge/Machine-Learning-01D277?style=flat&logoColor=white) ![Deep Learning](https://img.shields.io/badge/Deep-Learning-FAB040?style=flat&logoColor=white) ![Natural Language Processing](https://img.shields.io/badge/Natural%20Language%20Processing-4C8CBF?style=flat&logoColor=white) ![Software Development](https://img.shields.io/badge/Software%20Development-FF6600?style=flat&logoColor=white)|
| **Language**| ![Python](https://img.shields.io/badge/Python-4C8CBF?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2NCIgaGVpZ2h0PSI2NCIgdmlld0JveD0iMCAwIDMyIDMyIj48ZGVmcz48bGluZWFyR3JhZGllbnQgaWQ9IkEiIHgxPSI4MTEuNTI3IiB5MT0iNTc0Ljg5NSIgeDI9IjY2NS4yNTUiIHkyPSI1NzMuNzMyIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSI+PHN0b3Agb2Zmc2V0PSIwIiBzdG9wLWNvbG9yPSIjMzY2YTk2Ii8+PHN0b3Agb2Zmc2V0PSIxIiBzdG9wLWNvbG9yPSIjMzY3OWIwIi8+PC9saW5lYXJHcmFkaWVudD48bGluZWFyR3JhZGllbnQgaWQ9IkIiIHgxPSI4NjIuODI0IiB5MT0iNjQyLjE3NiIgeDI9IjU3My4yNzYiIHkyPSI2NDIuMTc2IiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSI+PHN0b3Agb2Zmc2V0PSIwIiBzdG9wLWNvbG9yPSIjZmZjODM2Ii8+PHN0b3Agb2Zmc2V0PSIxIiBzdG9wLWNvbG9yPSIjZmZlODczIi8+PC9saW5lYXJHcmFkaWVudD48L2RlZnM+PGcgdHJhbnNmb3JtPSJtYXRyaXgoLjE2MTcgMCAwIC4xNTgwODkgLTEwNy41Mzc2NCAtODEuNjYxODcpIj48cGF0aCBkPSJNNzE2LjI1NSA1NDQuNDg3YzAtMTMuNjIzIDMuNjUzLTIxLjAzNCAyMy44MjItMjQuNTYzIDEzLjY5My0yLjQgMzEuMjUtMi43IDQ3LjYyNyAwIDEyLjkzNSAyLjEzNSAyMy44MjIgMTEuNzcgMjMuODIyIDI0LjU2M3Y0NC45NDVjMCAxMy4xODItMTAuNTcgMjMuOTgtMjMuODIyIDIzLjk4aC00Ny42MjdjLTE2LjE2NCAwLTI5Ljc4NyAxMy43ODItMjkuNzg3IDI5LjM2M3YyMS41NjRoLTE2LjM3NmMtMTMuODUyIDAtMjEuOTE3LTkuOTg4LTI1LjMwNS0yMy45NjQtNC41Ny0xOC43NzYtNC4zNzYtMjkuOTYzIDAtNDcuOTQ1IDMuNzk0LTE1LjY4NyAxNS45MTctMjMuOTY0IDI5Ljc3LTIzLjk2NGg2NS41MnYtNmgtNDcuNjQ1di0xNy45OHoiIGZpbGw9InVybCgjQSkiLz48cGF0aCBkPSJNODExLjUyNyA2ODguMzJjMCAxMy42MjMtMTEuODIzIDIwLjUyMy0yMy44MjIgMjMuOTY0LTE4LjA1MiA1LjE4OC0zMi41NCA0LjM5NC00Ny42MjcgMC0xMi42LTMuNjctMjMuODIyLTExLjE3LTIzLjgyMi0yMy45NjR2LTQ0Ljk0NWMwLTEyLjkzNSAxMC43ODItMjMuOTggMjMuODIyLTIzLjk4aDQ3LjYyN2MxNS44NjQgMCAyOS43ODctMTMuNzEgMjkuNzg3LTI5Ljk2M3YtMjAuOTY0aDE3Ljg1OGMxMy44NyAwIDIwLjQgMTAuMzA1IDIzLjgyMiAyMy45NjQgNC43NjQgMTguOTcgNC45NzYgMzMuMTU3IDAgNDcuOTQ1LTQuODE3IDE0LjM2NC05Ljk3IDIzLjk2NC0yMy44MjIgMjMuOTY0SDc2My45djZoNDcuNjI3djE3Ljk4eiIgZmlsbD0idXJsKCNCKSIvPjxwYXRoIGQ9Ik03MjguMTY2IDU0MS41MDVjMC00Ljk3NiAzLjk4OC05IDguOTMtOSA0LjkyMyAwIDguOTMgNC4wMjMgOC45MyA5IDAgNC45Ni00LjAwNiA4Ljk4Mi04LjkzIDguOTgyLTQuOTQgMC04LjkzLTQuMDIzLTguOTMtOC45ODJ6bTUzLjU5IDE0OS43OThjMC00Ljk2IDQuMDA2LTguOTgyIDguOTMtOC45ODIgNC45NCAwIDguOTMgNC4wMjMgOC45MyA4Ljk4MiAwIDQuOTc2LTMuOTg4IDktOC45MyA5LTQuOTIzIDAtOC45My00LjAyMy04LjkzLTl6IiBmaWxsPSIjZmZmIi8+PC9nPjwvc3ZnPg==)|
| **Data Analysis**| ![NumPy](https://img.shields.io/badge/-NumPy-4C8CBF?style=flat&logo=numpy&logoColor=white) ![Pandas](https://img.shields.io/badge/-Pandas-000077?style=flat&logo=pandas&logoColor=white) ![SciPy](https://img.shields.io/badge/-SciPy%20-white?style=flat&logo=scipy&logoColor=004091) ![Matplotlib](https://img.shields.io/badge/matplotlib-white?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjgiIGhlaWdodD0iMTI4IiBzdHJva2U9IiM3NzciIGZpbGwtb3BhY2l0eT0iLjgiPgo8cGF0aCBmaWxsPSIjRkZGIiBkPSJtNjMsMWE2Myw2MyAwIDEsMCAyLDB6bTAsMTRhNDksNDkgMCAxLDAgMiwwem0wLDE0YTM1LDM1IDAgMSwwCjIsMHptMCwxNGEyMSwyMSAwIDEsMCAyLDB6bTAsMTRhNyw3IDAgMSwwIDIsMHptNjQsN0gxbTEwOC00NS05MCw5MG05MCwwLTkwLTkwbTQ1LTE4djEyNiIvPgo8cGF0aCBmaWxsPSIjRjYwIiBkPSJtNTAsOC0yMCwxMCA2OCw5MiAxMC0xMEw2NCw2NHoiLz4KPHBhdGggZmlsbD0iI0ZDMCIgZD0ibTE3LDUwdjI4TDY0LDY0eiIvPgo8cGF0aCBmaWxsPSIjN0Y3IiBkPSJtNjQsNjQgNiwzNUg1OHoiLz4KPHBhdGggZmlsbD0iI0NGMyIgZD0ibTY0LDY0IDEzLTQwIDksNXoiLz4KPHBhdGggZmlsbD0iIzA0RiIgZD0ibTY0LDY0IDE0LTYgMSw0emwtMjYsMTMgMyw0eiIvPgo8L3N2Zz4=)|
| **Databases**| ![MySQL](https://img.shields.io/badge/MySQL-white?logo=data:image/svg+xml;base64,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)|
| **Self-developed package**| [![mlforce](https://img.shields.io/badge/mlforce-green)](https://pypi.org/project/mlforce/) ![Machine Learning Force](https://img.shields.io/pypi/v/mlforce)|
| **Statistic Learning Tools**| ![R programming](https://img.shields.io/badge/R-525252?style=for-the-badge&logo=R) ![R Studio](https://img.shields.io/badge/RStudio-27338e?style=for-the-badge&logo=RStudio&logoColor=white)             |
| **Machine Learning Libraries** |   ![Scikit-learn](https://img.shields.io/badge/Scikit--Learn-white?logo=data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Generator: Adobe Illustrator 14.0.0, SVG Export Plug-In . SVG Version: 6.00 Build 43363)  -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   version="1.1"
   id="Layer_1"
   x="0px"
   y="0px"
   width="73.374084mm"
   height="39.570763mm"
   viewBox="0 0 277.31937 149.55879"
   enable-background="new 0 0 792 612"
   xml:space="preserve"
   inkscape:version="0.92.3 (2405546, 2018-03-11)"
   sodipodi:docname="scikit learn logo small.svg"><metadata
   id="metadata35"><rdf:RDF><cc:Work
       rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type
         rdf:resource="http://purl.org/dc/dcmitype/StillImage" /><dc:title></dc:title></cc:Work></rdf:RDF></metadata><defs
   id="defs33" /><sodipodi:namedview
   pagecolor="#ffffff"
   bordercolor="#666666"
   borderopacity="1"
   objecttolerance="10"
   gridtolerance="10"
   guidetolerance="10"
   inkscape:pageopacity="0"
   inkscape:pageshadow="2"
   inkscape:window-width="777"
   inkscape:window-height="626"
   id="namedview31"
   showgrid="false"
   inkscape:zoom="1.0907007"
   inkscape:cx="121.6884"
   inkscape:cy="79.49229"
   inkscape:window-x="2951"
   inkscape:window-y="679"
   inkscape:window-maximized="0"
   inkscape:current-layer="Layer_1"
   units="mm"
   showborder="true"
   fit-margin-top="0"
   fit-margin-left="0"
   fit-margin-right="0"
   fit-margin-bottom="0" />
<g
   id="g3"
   transform="translate(-120.60861,-220.26017)">
	<path
   d="m 333.32,347.348 c 33.869,-33.867 39.498,-83.146 12.572,-110.07 -26.922,-26.921 -76.199,-21.293 -110.066,12.572 -33.867,33.866 -24.07,98.568 -12.57,110.07 9.293,9.293 76.199,21.293 110.064,-12.572 z"
   id="path5"
   inkscape:connector-curvature="0"
   style="fill:#f89939" />
	<path
   d="m 194.35,298.411 c -19.648,-19.648 -48.242,-22.919 -63.867,-7.295 -15.621,15.622 -12.355,44.22 7.297,63.865 19.652,19.654 57.195,13.969 63.863,7.295 5.396,-5.387 12.361,-44.215 -7.293,-63.865 z"
   id="path7"
   inkscape:connector-curvature="0"
   style="fill:#3499cd" />
</g>
<g
   id="g9"
   transform="translate(-120.60861,-220.26017)">
	<g
   id="g11">
		<path
   d="m 262.143,339.047 c -3.471,3.195 -6.516,5.553 -9.133,7.068 -2.617,1.52 -5.113,2.279 -7.488,2.279 -2.732,0 -4.936,-1.059 -6.607,-3.178 -1.674,-2.121 -2.508,-4.965 -2.508,-8.543 0,-5.361 1.162,-11.797 3.486,-19.301 2.32,-7.51 5.145,-14.43 8.463,-20.761 l 9.729,-3.602 c 0.305,-0.102 0.537,-0.154 0.691,-0.154 0.738,0 1.348,0.544 1.816,1.627 0.473,1.088 0.711,2.55 0.711,4.388 0,5.209 -1.199,10.252 -3.602,15.129 -2.402,4.879 -6.154,10.086 -11.26,15.627 -0.205,2.656 -0.307,4.48 -0.307,5.477 0,2.223 0.408,3.982 1.225,5.285 0.818,1.305 1.902,1.953 3.256,1.953 1.381,0 2.848,-0.494 4.406,-1.49 1.555,-0.998 3.93,-3.064 7.121,-6.207 v 4.403 z m -14.668,-14.973 c 3.242,-3.605 5.875,-7.648 7.891,-12.121 2.016,-4.475 3.023,-8.324 3.023,-11.549 0,-0.94 -0.139,-1.704 -0.418,-2.278 -0.281,-0.575 -0.641,-0.864 -1.074,-0.864 -0.941,0 -2.316,2.352 -4.117,7.057 -1.801,4.704 -3.569,11.29 -5.305,19.755 z"
   id="path13"
   inkscape:connector-curvature="0"
   style="fill:#010101" />
		<path
   d="m 290.795,339.047 c -3.242,3.195 -6.152,5.553 -8.732,7.068 -2.58,1.52 -5.424,2.279 -8.541,2.279 -3.473,0 -6.275,-1.111 -8.41,-3.33 -2.131,-2.225 -3.195,-5.146 -3.195,-8.773 0,-5.412 1.875,-10.309 5.633,-14.688 3.75,-4.381 7.914,-6.57 12.484,-6.57 2.375,0 4.275,0.615 5.707,1.84 1.43,1.227 2.145,2.834 2.145,4.826 0,5.287 -5.617,9.574 -16.852,12.869 1.02,4.977 3.688,7.469 8.004,7.469 1.686,0 3.293,-0.453 4.824,-1.357 1.535,-0.908 3.844,-2.922 6.934,-6.035 v 4.402 z m -20.07,-7.084 c 6.535,-1.84 9.805,-5.234 9.805,-10.188 0,-2.451 -0.895,-3.676 -2.68,-3.676 -1.686,0 -3.293,1.281 -4.824,3.85 -1.536,2.565 -2.301,5.901 -2.301,10.014 z"
   id="path15"
   inkscape:connector-curvature="0"
   style="fill:#010101" />
		<path
   d="m 331.701,339.047 c -4.086,3.881 -7.01,6.412 -8.77,7.588 -1.762,1.174 -3.447,1.76 -5.057,1.76 -4.035,0 -5.936,-3.561 -5.707,-10.686 -2.553,3.65 -4.91,6.344 -7.068,8.084 -2.156,1.736 -4.383,2.602 -6.684,2.602 -2.244,0 -4.152,-1.051 -5.725,-3.158 -1.573,-2.107 -2.354,-4.691 -2.354,-7.758 0,-3.828 1.051,-7.48 3.156,-10.955 2.109,-3.473 4.809,-6.279 8.102,-8.424 3.293,-2.145 6.207,-3.219 8.732,-3.219 3.193,0 5.428,1.469 6.705,4.404 l 7.828,-4.326 h 2.148 l -3.381,11.221 c -1.736,5.645 -2.607,9.514 -2.607,11.607 0,2.195 0.777,3.293 2.336,3.293 0.992,0 2.09,-0.529 3.291,-1.59 1.201,-1.061 2.883,-2.676 5.053,-4.846 v 4.403 z m -28.037,2.109 c 2.553,0 4.959,-2.176 7.223,-6.529 2.26,-4.355 3.389,-8.373 3.389,-12.049 0,-1.428 -0.322,-2.547 -0.957,-3.35 -0.641,-0.807 -1.496,-1.207 -2.566,-1.207 -2.555,0 -4.977,2.17 -7.258,6.512 -2.285,4.342 -3.43,8.338 -3.43,11.986 0,1.381 0.34,2.498 1.016,3.354 0.676,0.856 1.534,1.283 2.583,1.283 z"
   id="path17"
   inkscape:connector-curvature="0"
   style="fill:#010101" />
		<path
   d="m 360.314,339.047 c -6.41,6.281 -11.352,9.424 -14.824,9.424 -1.559,0 -2.875,-0.658 -3.945,-1.969 -1.07,-1.316 -1.609,-2.945 -1.609,-4.887 0,-3.6 1.93,-8.424 5.785,-14.477 -1.891,0.971 -3.957,1.645 -6.205,2.029 -1.66,3.064 -4.266,6.359 -7.814,9.879 h -0.879 v -3.443 c 1.99,-2.068 3.791,-4.291 5.4,-6.666 -2.199,-0.971 -3.295,-2.414 -3.295,-4.326 0,-1.969 0.668,-4.068 2.012,-6.305 1.34,-2.232 3.184,-3.348 5.535,-3.348 1.992,0 2.986,1.018 2.986,3.062 0,1.609 -0.574,3.906 -1.725,6.895 4.238,-0.461 7.941,-3.701 11.109,-9.729 l 3.484,-0.154 -3.562,9.805 c -1.48,4.137 -2.438,6.955 -2.871,8.447 -0.433,1.492 -0.652,2.816 -0.652,3.963 0,1.074 0.25,1.932 0.746,2.566 0.498,0.643 1.17,0.959 2.012,0.959 0.918,0 1.801,-0.314 2.643,-0.936 0.842,-0.631 2.732,-2.359 5.67,-5.193 v 4.404 z"
   id="path19"
   inkscape:connector-curvature="0"
   style="fill:#010101" />
		<path
   d="m 397.928,339.047 c -5.898,6.234 -10.957,9.348 -15.168,9.348 -1.711,0 -3.09,-0.6 -4.137,-1.801 -1.049,-1.199 -1.572,-2.807 -1.572,-4.824 0,-2.732 1.125,-6.908 3.373,-12.523 1.199,-3.014 1.801,-4.932 1.801,-5.746 0,-0.818 -0.322,-1.227 -0.957,-1.227 -0.357,0 -0.832,0.18 -1.418,0.535 -0.539,0.357 -1.164,0.859 -1.879,1.496 -0.637,0.586 -1.354,1.301 -2.145,2.141 -0.691,0.721 -1.432,1.537 -2.219,2.453 l -2.148,2.492 c -0.943,1.148 -1.531,2.359 -1.76,3.637 -0.385,2.17 -0.639,4.164 -0.768,5.979 -0.078,1.35 -0.115,3.174 -0.115,5.477 l -8.465,1.988 c -0.279,-3.447 -0.422,-6.014 -0.422,-7.697 0,-4.111 0.479,-8.006 1.438,-11.682 0.957,-3.68 2.494,-7.814 4.615,-12.412 l 9.344,-1.799 c -1.965,5.287 -3.254,9.447 -3.867,12.484 4.188,-4.672 7.508,-7.906 9.969,-9.709 2.457,-1.801 4.645,-2.697 6.557,-2.697 1.299,0 2.385,0.49 3.25,1.471 0.869,0.982 1.301,2.215 1.301,3.689 0,2.449 -1.098,6.484 -3.291,12.104 -1.508,3.854 -2.262,6.355 -2.262,7.51 0,1.537 0.627,2.305 1.881,2.305 1.867,0 4.891,-2.465 9.064,-7.393 z"
   id="path21"
   inkscape:connector-curvature="0"
   style="fill:#010101" />
	</g>
</g>

<text
   font-size="23.0795"
   id="text25"
   style="font-size:23.0795002px;line-height:0%;font-family:Helvetica;fill:#ffffff"
   x="153.33279"
   y="81.945938">scikit</text>






</svg>)|
| **Deep Learning Frameworks** |  ![PyTorch](http://img.shields.io/badge/-PyTorch-4F004F?style=flat-square&logo=pytorch&logoColor=FF0000) ![TensorFlow](http://img.shields.io/badge/-TensorFlow-eee?style=flat-square&logo=tensorflow&logoColor=FF6F00) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-929292) |
| **Visualization techniques**| ![Tableau](https://img.shields.io/badge/-Tableau-FF0000?style=flat&logo=tableau&logoColor=white) ![PowerBI](https://img.shields.io/badge/-PowerBI-FF6F00?style=flat&logo=powerbi&logoColor=white) ![D3](https://img.shields.io/badge/-D3.js%20-EEEE00?style=flat&logo=d3.js&logoColor=white) ![Tulip](https://img.shields.io/badge/-Tulip%20-00AA00?style=flat&logo=Tulip&logoColor=white) ![yEd](https://img.shields.io/badge/-yEd%20-00000FF?style=flat&logo=yEd&logoColor=white) ![Gephi](https://img.shields.io/badge/-Gephi%20-4F004F?style=flat&logo=Gephid&logoColor=white)

## Curriculum Vitae
[English](assets/Jiarui_XU_CV.pdf) | [ä¸­æ–‡ç‰ˆ](assets/Jiarui_XU_CV(zh_CN).pdf)

## Projects / Experience(s)
- Currently working on : LLM Applications
- Internship(s):
  - Dec, 2023 - Feb, 2024:    
    - `AIGC Algorithm Intern` @![FunPlus](https://img.shields.io/badge/FunPlus-orange?logo=data:image/svg+xml;base64,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) 
    - `Depth Estimation`, `Retrieval-augmented Generation`, etc.

- [NumPy-Based Projects](#numpy-based-projects)
  - [Self-Developed Library](#self-developed-library-using-numpy)
  - [Multilayer Perceptron from Scratch](#multilayer-perceptron-from-scratch-using-numpy)
  - [Non-negative Matrix Factorization](#non-negative-matrix-factorization-using-numpy)
- [PyTorch-Based Projects](#pytorch-based-projects)
  - [Handwritten Character Recognition](#emnist-handwritten-character-classification)
  - [Image-Text Multimodal Classification](#cat---a-visual-text-multimodal-classifier)
  - [Robust Trainers for Noisy Labels](#robust-traniners-for-noisy-labels)
- [TensorFlow-Based Projects](#tensorflow-based-projects)
  - [Awesome Tutorials for TensorFlow2](#awesome-tutorials-for-tensorflow2)

## NumPy-Based Projects

![](https://img.shields.io/badge/-NumPy-blue?style=flat&logo=numpy) 

### :star: Self-Developed Library using NumPy
Various NumPy-based projects have been successfully integrated into my own open-source Python library, named [`MLForce`](https://pypi.org/project/mlforce/). This library is readily accessible on [GitHub](https://github.com/XavierSpycy/MLForce) and the [PyPI Community](https://pypi.org/project/mlforce).

<p align="center">
  <img src="assets/google-deepmind.jpg">
</p>
Photo by <a href="https://unsplash.com/@googledeepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Google DeepMind</a> on <a href="https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>

### :star: [Multilayer Perceptron from Scratch using NumPy](https://github.com/XavierSpycy/NumPyMultilayerPerceptron)
This project embodies a robust implementation of multilayer perceptron classifiers, entirely built upon the powerful NumPy library. We have successfully demonstrated its efficacy in our own unique task. Moving forward, our primary objective is to gradually enhance our model's versatility, ensuring it operates optimally across a diverse array of use cases.

Advantages of our implementation:
- Easy to construct:
```python
layers = [
    Input(input_dim=2),
    Dense(units=4, activation='leaky_relu', init='kaiming_normal', init_params={'mode': 'out'}),
    Dense(units=3, activation='hardswish', init='xavier_normal'),
    Dense(units=2, activation='relu', init='kaiming_normal', init_params={'mode': 'in'}),
    Dense(units=1, activation='tanh', init='xavier_uniform')
]
mlp = MultilayerPerceptron(layers)
```
- Easy and stable to train
```python
mlp.compile(optimizer='Adam',
            metrics=['MeanSquareError'])
mlp.fit(X, y, epochs=3, batch_size=8, use_progress_bar=True)
```
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/toy_loss.png">
</p>
<div align="center" style="font-weight: bold;">
  Loss
</div>

- Great results
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/toy_decision_boundary.png">
</p>
<div align="center" style="font-weight: bold;">
  Decision boundary
</div>

- Capability of dealing with complex datasets (10 classes, 128 features, 50,000 samples)
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/10classes.png">
</p>
<div align="center" style="font-weight: bold;">
  Smooth optimization procedure in 600 epochs
</div>

The architecture of this model is:
```python
layers = [
    Input(input_dim=128),
    Dense(units=120, activation='elu', init='kaiming_uniform'),
    Dropout(dropout_rate=0.25),
    Dense(units=112,  init='kaiming_uniform'),
    Dropout(dropout_rate=0.20),
    Dense(units=96, activation='elu', init='kaiming_uniform'),
    Dropout(dropout_rate=0.15),
    Dense(units=64, activation='elu', init='kaiming_uniform'),
    Dropout(dropout_rate=0.10),
    Dense(units=48, activation='elu', init='kaiming_uniform'),
    Dropout(dropout_rate=0.05),
    Dense(units=32, activation='elu', init='kaiming_uniform'),
    Dense(units=24, activation='elu', init='kaiming_uniform'),
    Dense(units=16, activation='elu', init='kaiming_uniform'),
    Dense(units=10, activation='softmax')
]
mlp = MultilayerPerceptron(layers)
optimizer = Adam(lr=1e-3, weight_decay=0.2)
scheduler = MultiStepLR(optimizer, milestones=[20, 40, 60, 80], gamma=0.8)
earlystopping = EarlyStopping(accuracy, patience=10, mode='max', restore_best_weights=True, start_from_epoch=20)
mlp.compile(optimizer=optimizer,
            metrics=['CrossEntropy', 'Accuracy'],
            scheduler=scheduler
)
mlp.fit(X_train, y_train, 
        epochs=90, batch_size=128, 
        validation_data=(X_test, y_test), use_progress_bar=True, 
        callbacks=[earlystopping]
)
```
### :star: [Non-negative Matrix Factorization using NumPy](https://github.com/XavierSpycy/NumPyNMF)
This project implements nine different Non-negative Matrix Factorization (NMF) algorithms and compares the robustness of each algorithm to five various types of noise in real-world data applications.

#### Well-reconstructed effects
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/pin.png">
</p>
<div align="center" style="font-weight: bold;">
  Image reconstruction
</div>

#### Sufficient experiments        
We conduct a seires of experiments, thus when developing your own algorithms, these results could act as a baseline. The results of the experiments (2 datasets $\times$ 5 noise types $\times$ 2 noise levels $\times$ 5 random seeds implicitly) are displayed in the repository.

- Flexible development      
Our development framework empowers you to effortlessly create your own NMF algorithms with minimal Python scripting:
```python
import numpy as np
from algorithm.nmf import BasicNMF

class ExampleNMF(BasicNMF):
    name = 'Example'
    # To tailor a unique NMF algorithm, subclass BasicNMF and redefine matrix_init and update methods.
    def matrix_init(self, X, n_components, random_state=None):
        # Implement your initialization logic here.
        # Although we provide built-in methods, crafting a bespoke initialization can markedly boost performance.
        # D, R = <your_initialization_logic>
        # D, R = np.array(D), np.array(R)
        return D, R  # Ensure D, R are returned.

    def update(self, X, **kwargs):
        # Implement the logic for iterative updates here.
        # Modify self.D, self.R as per your algorithm's logic.
        # flag = <convergence_criterion>
        return flag  # Return True if converged, else False.
```

#### Mature pipeline     
Our framework offers well-established pipelines, accommodating both standard and customized NMF tests. To utilize our existing NMF implementations, simply integrate them into our pipeline as demonstrated below:
```python
from algorithm.pipeline import Pipeline

pipeline = Pipeline(nmf='L1NormRegularizedNMF', 
                    dataset='ORL',
                    reduce=1,
                    noise_type='uniform',
                    noise_level=0.02,
                    random_state=3407, 
                    scaler='MinMax')
# Run the pipeline
pipeline.execute()
pipeline.evaluate()
```

For personalized NMF models, the `nmf` parameter accepts a `BasicNMF` object. You can seamlessly insert your own NMF model into our pipeline to evaluate its performance:
```python
pipeline = Pipeline(nmf=ExampleNMF(),
                    # Insert remaining configurations
)
```

#### Multiprocessing experiments (Latest release)

:rocket: Latest Release: We've harnessed the power of multiprocessing for extensive experiments, significantly enhancing efficiency. This approach has halved the overall experiment duration, reducing it to <u>at most 50%</u> of the time it would take to run each experiment sequentially.

For a comprehensive analysis of your algorithm, our platform enables conducting multiple experiments across various datasets:

```python
from algorithm.pipeline import Experiment

exp = Experiment()
# Once you build the data container
# You can choose an NMF algorithm and execute the experiment
exp.choose('L1NormRegularizedNMF')
# This step is very time-consuming, please be patient.
# If you achieve a better performance, congratulations! 
# You can share your results with us.
# Similarly, you can replace 'L1NormRegularizedNMF' with other your customized NMF algorithm
exp.execute()
```

#### Interactive algorithm interface (Latest release)
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/app_screen_shoot.jpg">
</p>
<p align="center">
  <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/app_screen_shoot_2.jpg">
</p>
<div align="center" style="font-weight: bold;">
  Demo
</div>

Note that the initial parameter in these experiments can also be a `BasicNMF` object, allowing the direct integration of your custom NMF model for thorough evaluation and testing.

**DON'T HESITATE TO DEVELOP YOUR OWN ALGORITHM!!!**

## PyTorch-Based Projects

![PyTorch](http://img.shields.io/badge/-PyTorch-4F004F?style=flat-square&logo=pytorch&logoColor=FF0000) 

<p align="center">
  <img src="assets/alex-knight-machinelearning.jpg">
</p>

Photo by <a href="https://unsplash.com/@agk42?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Alex Knight</a> on <a href="https://unsplash.com/photos/white-robot-near-brown-wall-2EJCSULRwC8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
  
### :star: [EMNIST Handwritten Character Classification](https://github.com/XavierSpycy/EMNIST-Classifier)
This project aims to reproduce various convolutional neural networks and modify them to our specific requirements.

We trained our models on a subset of the original datasets, which accounted for approximately 12% of the data. Surprisingly, our pre-trained models demonstrated remarkable generalization capabilities. During testing on the entire dataset, they exhibited excellent performance, showcasing their ability to achieve similar results when trained on just 10% of the data as compared to training on 100%.

Moreover, our models have proven to be transferable to downstream tasks, such as the MNIST datasets. By employing the architectures we implemented, we were able to attain an accuracy of over 99% on both the PyTorch and Kaggle MNIST datasets. Furthermore, by partially utilizing the trained parameters and unfreezing the parameters of the fully connected layers, we achieved an impressive accuracy of over 95%.

<div align="center">
<table style="text-align: center;">
  <caption>Performance of different CNNs on the training set</caption>
  <tr>
    <td></td>
    <td align="center">AlexNet</td>
    <td align="center">VGGNet</td>
    <td align="center">SpinalNet</td>
    <td align="center">ResNet</td>
  </tr>
  <tr>
    <td align="center">Accuracy</td>
    <td align="center">87.95%</td>
    <td align="center">89.80%</td>
    <td align="center">87.15%</td>
    <td align="center">89.28%</td>
  </tr>
  <tr>
    <td align="center">Precision</td>
    <td align="center">87.62%</td>
    <td align="center">90.01%</td>
    <td align="center">86.18%</td>
    <td align="center">89.24%</td>
  </tr>
  <tr>
    <td align="center">Recall</td>
    <td align="center">87.95%</td>
    <td align="center">89.80%</td>
    <td align="center">87.15%</td>
    <td align="center">89.28%</td>
  </tr>
  <tr>
    <td align="center">F1 score</td>
    <td align="center">86.59%</td>
    <td align="center">88.42%</td>
    <td align="center">85.28%</td>
    <td align="center">88.30%</td>
  </tr>
</table>
</div>

<div align="center">
<table style="text-align: center;">
  <caption>Performance of different CNNs on the test set</caption>
  <tr>
    <td></td>
    <td align="center">AlexNet</td>
    <td align="center">VGGNet</td>
    <td align="center">SpinalNet</td>
    <td align="center">ResNet</td>
  </tr>
  <tr>
    <td align="center">Accuracy</td>
    <td align="center">86.96%</td>
    <td align="center">87.24%</td>
    <td align="center">85.92%</td>
    <td align="center">86.88%</td>
  </tr>
  <tr>
    <td align="center">Precision</td>
    <td align="center">85.55%</td>
    <td align="center">86.43%</td>
    <td align="center">85.92%</td>
    <td align="center">86.88%</td>
  </tr>
  <tr>
    <td align="center">Recall</td>
    <td align="center">86.96%</td>
    <td align="center">87.24%</td>
    <td align="center">85.92%</td>
    <td align="center">86.88%</td>
  </tr>
  <tr>
    <td align="center">F1 score</td>
    <td align="center">85.58%</td>
    <td align="center">85.66%</td>
    <td align="center">84.07%</td>
    <td align="center">85.68%</td>
  </tr>
</table>
</div>

<p align="center">
  <img src="https://github.com/XavierSpycy/EMNIST-Classifier/blob/main/outputs/predictions_short.png">
</p>
<div align="center" style="font-weight: bold;">
  Effects of one model
</div>


### :star: [CAT - A Visual-Text Multimodal Classifier](https://github.com/XavierSpycy/CAT-ImageTextIntegrator)
This project involves a multi-label multi-classification problem. We deployed four pre-trained image models and two pre-trained text models. To enhance performance, we developed 12 multi-modal models using self-attention and cross-attention mechanisms. The project poster showcases some valuable techniques and intriguing discoveries.

<p align="center">
  <img src="https://github.com/XavierSpycy/CAT-ImageTextIntegrator/blob/main/outcomes/CAT-2.jpeg">
</p>
<div align="center" style="font-weight: bold;">
  CAT (Convolution, Attention and Transformer) architecture
</div>

<p align="center">
  <img src="https://github.com/XavierSpycy/CAT-ImageTextIntegrator/blob/main/outcomes/poster.jpg">
</p>
<div align="center" style="font-weight: bold;">
  Project Poster
</div>

### :star: [Robust Traniners for Noisy Labels](https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels)
This project is an experimental repository focusing on dealing with datasets containing a high level of noisy labels (50% and above). This repository features experiments conducted on the `FashionMNIST` and `CIFAR` datasets using the `ResNet34` as the baseline classifier.

The repository explores various training strategies ('trainers'), including `ForwardLossCorrection`, `CoTeaching`, `JoCoR`, and `O2UNet`. Specifically, for datasets with unknown transition matrices, `DualT` is employed as the Transition Matrix Estimator. Given the computational complexity and practical performance considerations, our experiments primarily focus on `ForwardLossCorrection` and `CoTeaching`. We conducted multiple experiments with different random seeds to compare these two methods.

Initial explorations on `FashionMNIST0.5` with `JoCoR` and `O2UNet` have shown promising results. This repository serves as a resource for those interested in robust machine learning techniques under challenging conditions of high label noise.

- Meaningful Loss Trends

<p align="center">
  <img src="https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels/blob/main/figures/loss_correction_trend.png">
</p>
<div align="center" style="font-weight: bold;">
  Loss Trend 1
</div>

<p align="center">
  <img src="https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels/blob/main/figures/co_teaching_trend.png">
</p>
<div align="center" style="font-weight: bold;">
  Loss Trend 2
</div>

- Persuasive Results
<div align="center">
    <h5>Actual Transition Matrix of FashionMNIST0.5</h5>
    <table>
    <tr><td>0.5</td><td>0.2</td><td>0.3</td></tr>
    <tr><td>0.3</td><td>0.5</td><td>0.2</td></tr>
    <tr><td>0.2</td><td>0.3</td><td>0.5</td></tr>
    </table>
</div>

<div align="center">
    <h5>Estimated Transition Matrix of FashionMNIST0.5</h5>
    <table>
    <tr><td>0.473</td><td>0.209</td><td>0.309</td></tr>
    <tr><td>0.306</td><td>0.485</td><td>0.232</td></tr>
    <tr><td>0.221</td><td>0.306</td><td>0.460</td></tr>
    </table>
</div>

<div align="center">
    <h5>Actual Transition Matrix of FashionMNIST0.6</h5>
    <table>
    <tr><td>0.4</td><td>0.3</td><td>0.3</td></tr>
    <tr><td>0.3</td><td>0.4</td><td>0.3</td></tr>
    <tr><td>0.3</td><td>0.3</td><td>0.4</td></tr>
    </table>
</div>

<div align="center">
    <h5>Estimated Transition Matrix of FashionMNIST0.6</h5>
    <table>
    <tr><td>0.407</td><td>0.295</td><td>0.298</td></tr>
    <tr><td>0.297</td><td>0.394</td><td>0.308</td></tr>
    <tr><td>0.301</td><td>0.310</td><td>0.388</td></tr>
    </table>
</div>

## TensorFlow-Based Projects
![TensorFlow](http://img.shields.io/badge/-TensorFlow-eee?style=flat-square&logo=tensorflow&logoColor=FF6F00) 
### :star: [Awesome Tutorials for TensorFlow2](https://github.com/XavierSpycy/Awesome-Tutorials-for-TensorFlow2)


## Certification:
![Coursera](https://img.shields.io/badge/Coursera-F9AB00?style=for-the-badge&logo=Coursera&color=525252)

**Natural Language Processing Specialization** - DeepLearning.AI, Oct 2023 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/MGL3LJNX3RZK)

**Deep Learning Specialization** - DeepLearning.AI, Aug 2023- [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/MCHWQ64ZN86Q)

**Mathematics for Machine Learning and Data Science Specialization** - DeepLearning.AI, Aug 2023- [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/UJQNSGBXB4FL)

**Applied Data Science with Python Specialization** - University of Michigan, Jul 2023 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/S77MYG6WQCUS)

**Machine Learning Specialization** - DeepLearning.AI, Stanford University, Jul 2023 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/GVJJK3HZRSLZ)

**Mathematics for Machine Learning Specialization** - Imperial College London, Jun 2023 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/UHPTL6AM2WHK)

**Expressway to Data Science: Python Programming Specialization** - University of Colorado Boulder, Dec 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/D9N32LL56ZE8)

**Python 3 Programming Specializationn** - University of Michigan, Dec 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/5ZZC4LJULZ83)

**Introduction to Scripting in Python Specializationn** - Rice University, Nov 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/KN3NPRDUTAFG)

**Statistics with Python Specialization** - University of Michigan, Nov 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/2GDKV3UYSTNU)

**Excel Skills for Data Analytics and Visualization Specialization** - Macquarie University, Oct 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/R9Y9AFUZURMK)

**Python for Everybody Specialization** - University of Michigan, Oct 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/P2L44FCJR9V6)

**Excel Skills for Business Specialization** - Macquarie University, Sep 2022 - [Badge](https://www.coursera.org/account/accomplishments/specialization/certificate/PKTDSLULS3SB)

![Wall](assets/wall_Aug_23_1.jpg)
![Wall](assets/wall_Aug_23_2.jpg)

## How to Reach me:
<p align="left">
    <a href="mailto:jixu9182@uni.sydney.edu.au" target="blank">
        <img align="center" src="assets/gmail.svg" alt="Gmail" height="30" width="30" style="margin-right:10px;"/>
        Email: jixu9182@uni.sydney.edu.au
    </a>
</p>
<p align="left">
    <a href="https://www.linkedin.com/in/jiarui-xu-xavierspycy98" target="blank">
        <img align="center" src="assets/linkedin.svg" alt="LinkedIn" height="30" width="30" style="margin-right:10px;"/>
        LinkedIn: Jiarui XU
    </a>
</p>

## Thank you for visiting :heart: