<img src="assets/Bottom_up.svg">

<img src="assets/chris-ried-python-crop.jpg">
Photo by <a href="https://unsplash.com/@cdr6934?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Chris Ried</a> on <a href="https://unsplash.com/photos/ieic5Tq8YMk?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>

<a><img src="https://upload.wikimedia.org/wikipedia/commons/c/c3/Python-logo-notext.svg" align="right" height="48" width="48" ></a>

<a href="https://git.io/typing-svg">
    <img src="https://readme-typing-svg.herokuapp.com?color=%2336BCF7&center=true&vCenter=true&width=600&lines=👋++Hi~,+I+am+Jiarui+Xu~;👋++你好～我叫徐嘉瑞～;Welcome+to+my+profile!;欢迎来到我的主页!;Master's+degree+of+Data+Science;数据科学硕士;Machine+learning+specialization;机器学习专项;Python+programming+enthusiast;Python编程爱好者;Patience+and+persistence.;持之以恒。" alt="Typing SVG">
</a>

```mermaid
graph TD;
    DomainKnowledge[Domain Knowledge]-->MachineLearning[Machine Learning];
    MachineLearning[Machine Learning]-->StatisticalLearning[Statistical Learning];
    MachineLearning[Machine Learning]-->DeepLearning[Deep Learning];
    DomainKnowledge[Domain Knowledge]-->BackendDevelopment[Backend Development];
    DeepLearning[Deep Learning]-->ImageClassification[Image Classification];
    ImageClassification[Image Classification]-->LabelNoise[LabelNoises];
    DeepLearning[Deep Learning]-->NaturalLanguageProcessing[Natural Language Processing];
    NaturalLanguageProcessing[Natural Language Processing]-->TopicModeling[Topic Modeling];
    NaturalLanguageProcessing[Natural Language Processing]-->LLMApplication[LLM Application];
    DeepLearning[Deep Learning]-->Multimodality[Multimodality];
    Multimodality[Multimodality]-->ImageTextClassification[Image-Text Classification];
    Multimodality[Multimodality]-->VisualQuestionAnswering[VisualQuestionAnswering];
   ```

| Properties|Skills|
|-|-|
| **Domain Knownledge**| ![](https://img.shields.io/badge/Machine-Learning-01D277?style=flat&logoColor=white&labelColor=ADD8E6) ![Deep Learning](https://img.shields.io/badge/Deep-Learning-01D277?style=flat&logoColor=white&labelColor=0000FF) ![Natural Language Processing](https://img.shields.io/badge/Natural%20Language%20Processing-4C8CBF?style=flat&logoColor=white) ![Software Development](https://img.shields.io/badge/Software%20Development-FF6600?style=flat&logoColor=white) ![Multimodality Learning](https://img.shields.io/badge/Multimodality-Learning-01D277?style=flat&labelColor=lightyellow)|
| **Language**| ![Python](https://img.shields.io/badge/Python-4C8CBF?logo=data:image/svg+xml;base64,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)|
| **Data Analysis**| ![NumPy](https://img.shields.io/badge/-NumPy-4C8CBF?style=flat&logo=numpy&logoColor=white) ![Pandas](https://img.shields.io/badge/-Pandas-000077?style=flat&logo=pandas&logoColor=white) ![SciPy](https://img.shields.io/badge/-SciPy%20-white?style=flat&logo=scipy&logoColor=004091) ![Matplotlib](https://img.shields.io/badge/matplotlib-white?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjgiIGhlaWdodD0iMTI4IiBzdHJva2U9IiM3NzciIGZpbGwtb3BhY2l0eT0iLjgiPgo8cGF0aCBmaWxsPSIjRkZGIiBkPSJtNjMsMWE2Myw2MyAwIDEsMCAyLDB6bTAsMTRhNDksNDkgMCAxLDAgMiwwem0wLDE0YTM1LDM1IDAgMSwwCjIsMHptMCwxNGEyMSwyMSAwIDEsMCAyLDB6bTAsMTRhNyw3IDAgMSwwIDIsMHptNjQsN0gxbTEwOC00NS05MCw5MG05MCwwLTkwLTkwbTQ1LTE4djEyNiIvPgo8cGF0aCBmaWxsPSIjRjYwIiBkPSJtNTAsOC0yMCwxMCA2OCw5MiAxMC0xMEw2NCw2NHoiLz4KPHBhdGggZmlsbD0iI0ZDMCIgZD0ibTE3LDUwdjI4TDY0LDY0eiIvPgo8cGF0aCBmaWxsPSIjN0Y3IiBkPSJtNjQsNjQgNiwzNUg1OHoiLz4KPHBhdGggZmlsbD0iI0NGMyIgZD0ibTY0LDY0IDEzLTQwIDksNXoiLz4KPHBhdGggZmlsbD0iIzA0RiIgZD0ibTY0LDY0IDE0LTYgMSw0emwtMjYsMTMgMyw0eiIvPgo8L3N2Zz4=)|
| **Databases**| ![MySQL](https://img.shields.io/badge/MySQL-white?logo=data:image/svg+xml;base64,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)|
| **Self-developed package**| [![mlforce](https://img.shields.io/badge/mlforce-green)](https://pypi.org/project/mlforce/) ![Machine Learning Force](https://img.shields.io/pypi/v/mlforce)|
| **Statistic Learning Tools**| ![R programming](https://img.shields.io/badge/R-525252?style=for-the-badge&logo=R) ![R Studio](https://img.shields.io/badge/RStudio-27338e?style=for-the-badge&logo=RStudio&logoColor=white)             |
| **Machine Learning Libraries** |   ![Scikit Learn](https://img.shields.io/badge/Scikit-Learn-blue?style=flat&logoColor=black&labelColor=orange)|
| **Deep Learning Frameworks** |  ![PyTorch](http://img.shields.io/badge/-PyTorch-4F004F?style=flat-square&logo=pytorch&logoColor=FF0000) ![TensorFlow](http://img.shields.io/badge/-TensorFlow-eee?style=flat-square&logo=tensorflow&logoColor=FF6F00) ![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-929292) |
| **Visualization techniques**| ![Tableau](https://img.shields.io/badge/-Tableau-FF0000?style=flat&logo=tableau&logoColor=white) ![PowerBI](https://img.shields.io/badge/-PowerBI-FF6F00?style=flat&logo=powerbi&logoColor=white) ![D3](https://img.shields.io/badge/-D3.js%20-EEEE00?style=flat&logo=d3.js&logoColor=white) ![Tulip](https://img.shields.io/badge/-Tulip%20-00AA00?style=flat&logo=Tulip&logoColor=white) ![yEd](https://img.shields.io/badge/-yEd%20-00000FF?style=flat&logo=yEd&logoColor=white) ![Gephi](https://img.shields.io/badge/-Gephi%20-4F004F?style=flat&logo=Gephid&logoColor=white)

## Curriculum Vitae
[English](assets/Jiarui_XU_CV.pdf) | [中文版](assets/Jiarui_XU_CV__zh_CN_.pdf)

<details>
  <summary><h2>Experiences / Projects</h2></summary>
  <ul>
    <li>
      Ressearch Interests : LLM Applications / Multimodal Learning
    </li>
    <li>
      Research Experiences:
      <ul>
        <li>
          <em>Top</em>ic <em>M</em>odeling for the <em>E</em>volution through <em>D</em>escriptions of <em>A</em>pplications <em>L</em>ongitudina (<em>TopMEDAL</em>)
        </li>
        <li>
          Cross-modal medical image representation learning (<u><em>VRI Scholarship</em></u>)
        </li>
      </ul>
    </li>
    <li>
      Internship(s):
      <ul>
        <li>
          <code>AIGC Algorithm Intern</code> @ <img src="https://img.shields.io/badge/FunPlus-orange?logo=data:image/svg+xml;base64,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">:
        </li>
        <ul>
          <li>
            2023.12 ~ 2024.02
          </li>
          <li>
            Deployed SOTA algorithms.
          </li>
          <li>
              Developed LLM applications with RAG.
          </li>
        </ul>
      </ul>
      <li>
        Projects
        <ul>
          <li><a href="#numpy-based-projects">NumPy-Based Projects</a>
            <ul>
              <li><a href="#star-self-developed-library-using-numpy">Self-Developed Library</a></li>
              <li><a href="#star-mlp">Multilayer Perceptron from Scratch</a></li>
              <li><a href="#star-nmf">Non-negative Matrix  Factorization Implementation</a></li>
            </ul>
        </li>
          <li><a href="#pytorch-based-projects">PyTorch-Based Projects</a>
            <ul>
                <li><a href="#star-emnist-handwritten-character-classification">Handwritten Character Recognition</a></li>
                <li><a href="#star-cat">CAT: A Visual-Text Multimodal Classifier</a></li>
                <li><a href="#star-noisy-labels">Robust Trainers for Noisy Labels</a></li>
                <li><a href="#star-multiclip">MultiCLIP: Multimodal-Multilabel-Multistage Classification using Language Image Pre-training</a>
            </ul>
          </li>
          <li><a href="#tensorflow-based-project">TensorFlow-Based Project</a>
            <ul>
              <li><a href="#star-tf2">Awesome Tutorials for TensorFlow2</a></li>
            </ul>
          </li>
        </ul>
      </li>
  </ul>
</details>

<details>
  <summary id="numpy-based-projects"><h2>NumPy-Based Projects</h2></summary>
  <img src="https://img.shields.io/badge/-NumPy-blue?style=flat&logo=numpy">
  <p align="center">
    <img src="assets/google-deepmind.jpg">
  </p>
  <p>
    Photo by <a href="https://unsplash.com/@googledeepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Google DeepMind</a> on <a href="https://unsplash.com/photos/LaKwLAmcnBc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
  </p>

  <details>
    <summary id="star-self-developed-library-using-numpy">
      <h3>🌟Self-Developed Library using NumPy</h3><a href="https://github.com/XavierSpycy/MLForce">&#x2197;</a>
    </summary>
    <p>
      My NumPy-based projects have been successfully integrated into my own open-source Python library, named <a href="https://pypi.org/project/mlforce/"><code>MLForce</code></a>. This library is also readily accessible on the <a href="https://pypi.org/project/mlforce">PyPI Community</a>.
    </p>
  </details>

  <details>
    <summary id="star-mlp">
      <h3>🌟Multilayer Perceptron from Scratch using NumPy<a href="https://github.com/XavierSpycy/NumPyMultilayerPerceptron">&#x2197;</a></h3>
      <p>
    </summary>
      A robust implementation of multilayer perceptrons, entirely built upon the powerful NumPy library.
      </p>
      <p>Advantages of our implementation:
        <ul>
          <li>
            <details>
              <summary>Easy to construct</summary>
              <p>
                <pre><code class="language-python">layers = [
    Input(input_dim=2),
    Dense(units=4, activation='leaky_relu', init='kaiming_normal', init_params={'mode': 'out'}),
    Dense(units=3, activation='hardswish', init='xavier_normal'),
    Dense(units=2, activation='relu', init='kaiming_normal', init_params={'mode': 'in'}),
    Dense(units=1, activation='tanh', init='xavier_uniform')
]
mlp = MultilayerPerceptron(layers)</code></pre>
              </p>
            </details>
          </li>
          <li>
            <details>
              <summary>Easy and stable to train</summary>
              <p>
                <pre><code class="language-python">mlp.compile(optimizer='Adam',
            metrics=['MeanSquareError'])
mlp.fit(X, y, epochs=3, batch_size=8, use_progress_bar=True)</code></pre>
              </p>
              <p align="center">
                <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/toy_loss.png">
              </p>
              <div align="center" style="font-weight: bold;">
                Loss
              </div>
            </details>
          </li>
          <li>
            <details>
              <summary>
                Great results
              </summary>
              <p align="center">
                <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/toy_decision_boundary.png">
              </p>
              <div align="center" style="font-weight: bold;">
                Decision boundary
              </div>
            </details>
          </li>
          <li>
            <details>
              <summary>
                Capability of dealing with complex datasets (10 classes, 128 features, 50,000 samples)
              </summary>
              <p align="center">
                <img src="https://github.com/XavierSpycy/NumPyMultilayerPerceptron/blob/main/figures/10classes.png">
              </p>
              <div align="center" style="font-weight: bold;">
                Smooth optimization procedure in 600 epochs
              </div>
            </details>
          </li>
  </details>

<details>
  <summary id="star-nmf">
    <h3>🌟Non-negative Matrix Factorization using NumPy<a href="https://github.com/XavierSpycy/NumPyNMF">&#x2197;</a></h3>
  </summary>
  <p>
    This project implements nine different Non-negative Matrix Factorization (NMF) algorithms and compares the robustness of each algorithm to five various types of noise in real-world data applications.
  </p>
  <ul>
    <li>
    <details>
      <summary>
        Well-reconstructed effects
      </summary>
      <p align="center">
        <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/pin.png">
      </p>
      <div align="center" style="font-weight: bold;">
        Image reconstruction
      </div>
    </details>
    </li>
    <li>
    <details>
      <summary>Sufficient experiments</summary>
      <p>
        We conduct a seires of experiments, thus when developing your own algorithms, these results could act as a baseline. The results of the experiments (2 datasets &times; 5 noise types &times; 2 noise levels &times; 5 random seeds implicitly) are displayed in the repository.
      </p>
    </details>
    </li>
    <li>
    <details>
      <summary>Flexible development</summary>
      <p>
        Our development framework empowers you to effortlessly create your own NMF algorithms with minimal Python scripting.
      </p>
    </details>
    </li>
    <li>
    <details>
      <summary>Mature pipeline</summary>
      <p>
        Our framework offers well-established pipelines, accommodating both standard and customized NMF tests. 
      </p>
      <p>
        For personalized NMF models, the <code>nmf</code> parameter accepts a <code>BasicNMF</code> object. You can seamlessly insert your own NMF model into our pipeline to evaluate its performance.
      </p>
    </details>
    </li>
    <li>
    <details>
      <summary>Multiprocessing experiments</summary>
      <p>
        We've harnessed the power of multiprocessing for extensive experiments, significantly enhancing efficiency. This approach has halved the overall experiment duration, reducing it to 30% ~ 50% of the time it would take to run each experiment sequentially.
      </p>
      <p>
        For a comprehensive analysis of your algorithm, our platform enables conducting multiple experiments across various datasets:
      </p>
      <pre><code>from algorithm.pipeline import Experiment

exp = Experiment()
exp.choose('L1NormRegularizedNMF')
exp.execute()</code></pre>
    </details>
    </li>
    <li>
    <details>
      <summary>Interactive algorithm interface</summary>
      <p align="center">
        <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/app_screen_shoot.jpg">
      </p>
      <p align="center">
        <img src="https://github.com/XavierSpycy/NumPyNMF/blob/main/figures/app_screen_shoot_2.jpg">
      </p>
      <div align="center" style="font-weight: bold;">
        Demo
      </div>
      <p>
        Note that the initial parameter in these experiments can also be <code>BasicNMF</code> object, allowing the direct integration of your custom NMF model for thorough evaluation and testing.
      </p>
    </details>
    </li>
  </ul>
<b>
  DON'T HESITATE TO DEVELOP YOUR OWN ALGORITHM!!!
</b>
</details>
</details>

<details>
  <summary id="pytorch-based-projects">
    <h2>PyTorch-Based Projects</h2>
  </summary>
  <img src="http://img.shields.io/badge/-PyTorch-4F004F?style=flat-square&logo=pytorch&logoColor=FF0000">
  <p align="center">
    <img src="assets/alex-knight-machinelearning.jpg">
  </p>

  Photo by <a href="https://unsplash.com/@agk42?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Alex Knight</a> on <a href="https://unsplash.com/photos/white-robot-near-brown-wall-2EJCSULRwC8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
  
  <details>
    <summary id="star-emnist-handwritten-character-classification">
      <h3>🌟EMNIST Handwritten Character Classification</h3><a href="https://github.com/XavierSpycy/EMNIST-Classifier">&#x2197;</a>
    </summary>
    <p>
      This project aims to reproduce various convolutional neural networks and modify them to our specific requirements.
    </p>
    <div align="center">
  <table style="text-align: center;">
    <caption>Performance of different CNNs on the training set</caption>
    <tr>
      <td></td>
      <td align="center">AlexNet</td>
      <td align="center">VGGNet</td>
      <td align="center">SpinalNet</td>
      <td align="center">ResNet</td>
    </tr>
    <tr>
      <td align="center">Accuracy</td>
      <td align="center">87.95%</td>
      <td align="center">89.80%</td>
      <td align="center">87.15%</td>
      <td align="center">89.28%</td>
    </tr>
    <tr>
      <td align="center">Precision</td>
      <td align="center">87.62%</td>
      <td align="center">90.01%</td>
      <td align="center">86.18%</td>
      <td align="center">89.24%</td>
    </tr>
    <tr>
      <td align="center">Recall</td>
      <td align="center">87.95%</td>
      <td align="center">89.80%</td>
      <td align="center">87.15%</td>
      <td align="center">89.28%</td>
    </tr>
    <tr>
      <td align="center">F1 score</td>
      <td align="center">86.59%</td>
      <td align="center">88.42%</td>
      <td align="center">85.28%</td>
      <td align="center">88.30%</td>
    </tr>
  </table>
  </div>

  <div align="center">
  <table style="text-align: center;">
    <caption>Performance of different CNNs on the test set</caption>
    <tr>
      <td></td>
      <td align="center">AlexNet</td>
      <td align="center">VGGNet</td>
      <td align="center">SpinalNet</td>
      <td align="center">ResNet</td>
    </tr>
    <tr>
      <td align="center">Accuracy</td>
      <td align="center">86.96%</td>
      <td align="center">87.24%</td>
      <td align="center">85.92%</td>
      <td align="center">86.88%</td>
    </tr>
    <tr>
      <td align="center">Precision</td>
      <td align="center">85.55%</td>
      <td align="center">86.43%</td>
      <td align="center">85.92%</td>
      <td align="center">86.88%</td>
    </tr>
    <tr>
      <td align="center">Recall</td>
      <td align="center">86.96%</td>
      <td align="center">87.24%</td>
      <td align="center">85.92%</td>
      <td align="center">86.88%</td>
    </tr>
    <tr>
      <td align="center">F1 score</td>
      <td align="center">85.58%</td>
      <td align="center">85.66%</td>
      <td align="center">84.07%</td>
      <td align="center">85.68%</td>
    </tr>
  </table>
  </div>

  <p align="center">
    <img src="https://github.com/XavierSpycy/EMNIST-Classifier/blob/main/outputs/predictions_short.png">
  </p>
  <div align="center" style="font-weight: bold;">
    Effects of VGGNet
  </div>
  </details>

<details>
  <summary id="star-cat">
    <h3>🌟CAT: A Visual-Text Multimodal Classifier</h3><a href="https://github.com/XavierSpycy/CAT-ImageTextIntegrator">&#x2197;</a>
  </summary>
  <p>
    This project involves a multi-label multi-classification problem. We deployed four pre-trained image models and two pre-trained text models. To enhance performance, we developed 12 multi-modal models using self-attention and cross-attention mechanisms. The project poster showcases some valuable techniques and intriguing discoveries.
  </p>
  <p align="center">
    <img src="https://github.com/XavierSpycy/CAT-ImageTextIntegrator/blob/main/outcomes/CAT-2.jpeg">
  </p>
  <div align="center" style="font-weight: bold;">
    CAT (Convolution, Attention and Transformer) architecture
  </div>

  <p align="center">
    <img src="https://github.com/XavierSpycy/CAT-ImageTextIntegrator/blob/main/outcomes/poster.jpg">
  </p>
  <div align="center" style="font-weight: bold;">
    Project Poster
  </div>
</details>

<details>
  <summary id="star-noisy-labels">
    <h3>🌟Robust Traniners for Noisy Labels</h3><a href="https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels">&#x2197;</a>
  </summary>
  <p>
    This project is an experimental repository focusing on dealing with datasets containing a high level of noisy labels (50% and above). This repository features experiments conducted on the <code>FashionMNIST</code> and <code>CIFAR</code> datasets using the <code>ResNet34</code> as the baseline classifier.
  </p>
  <p>
    The repository explores various training strategies (<code>Trainer</code> objects), including <code>ForwardLossCorrection</code>, <code>CoTeaching</code>, <code>JoCoR</code>, and <code>O2UNet</code>. Specifically, for datasets with unknown transition matrices, <code>DualT</code> is employed as the Transition Matrix Estimator.
  </p>
  <ul>
    <li>
      <details>
        <summary>Meaningful Loss Trends</summary>
        <p align="center">
          <img src="https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels/blob/main/figures/loss_correction_trend.png">
        </p>
        <div align="center" style="font-weight: bold;">
          Loss Trend 1
        </div>
        <p align="center">
          <img src="https://github.com/XavierSpycy/Robust-Trainers-for-Noisy-Labels/blob/main/figures/co_teaching_trend.png">
        </p>
        <div align="center" style="font-weight: bold;">
          Loss Trend 2
        </div>
      </details>
    </li>
    <li>
      <details>
        <summary>Persuasive Results</summary>
        <div align="center">
          <h5>FashionMNIST0.5</h5>
            <table>
              <tr>
                <th colspan="3">Actual Transition Matrix</th>
                <th colspan="3">Estimated Transition Matrix</th>
              </tr>
              <tr>
                <td>0.5</td><td>0.2</td><td>0.3</td><td>0.473</td><td>0.209</td><td>0.309</td>
              </tr>
              <tr>
                <td>0.3</td><td>0.5</td><td>0.2</td><td>0.306</td><td>0.485</td><td>0.232</td>
              </tr>
              <tr>
                <td>0.2</td><td>0.3</td><td>0.5</td><td>0.221</td><td>0.306</td><td>0.460</td>
              </tr>
            </table>
        </div>
        <div align="center">
          <h5>FashionMNIST0.6</h5>
            <table>
              <tr>
                  <th colspan="3">Actual Transition Matrix</th>
                  <th colspan="3">Estimated Transition Matrix</th>
              </tr>
              <tr>
                <td>0.4</td><td>0.3</td><td>0.3</td><td>0.407</td><td>0.295</td><td>0.298</td>
              </tr>
              <tr>
                <td>0.3</td><td>0.4</td><td>0.3</td><td>0.297</td><td>0.394</td><td>0.308</td>
              </tr>
              <tr>
                <td>0.3</td><td>0.3</td><td>0.4</td><td>0.301</td><td>0.310</td><td>0.388</td></tr>
              </tr>
            </table>
        </div>
      </details>
    </li>
  </ul>
</details>
<details>
  <summary>
    <h3>
      🌟Transformers for Tabular Data<a href="https://github.com/XavierSpycy/tabtransformers">&#x2197;</a>
    </h3>
  </summary>
  <p>
    A PyTorch-based implementation that leverages Transformer architectures to enhance the handling and design of tabular data.
  </p>
</details>
<details>
  <summary id="star-multiclip">
    <h3>🌟MultiCLIP: Multimodal-Multilabel-Multistage Classification using Language Image Pre-training<a href="https://github.com/XavierSpycy/MultiCLIP">&#x2197;</a></h3>
  </summary>
  <p>
    A framework for multimodal-multilabel-multistage classification utilizing advanced pretrained models like CLIP and BLIP. 
  </p>
  <p>
    Diagrams of implementation:
  </p>
  <p align="center">
    <img src="https://github.com/XavierSpycy/MultiCLIP/blob/main/figures/clip_router.jpg">
  </p>
  <div align="center" style="font-weight: bold">
    CLIP + Router
  </div>
  <p align="center">
    <img src="https://github.com/XavierSpycy/MultiCLIP/blob/main/figures/blip_ml_decoder.png">
  </p>
  <div align="center" style="font-weight: bold">
    BLIP + Anything
  </div>
</details>
</details>

<details>
  <summary id="tensorflow-based-project">
    <h2>TensorFlow-Based Project</h2>
  </summary>
    <details>
      <summary id="star-tf2">
        <h3>🌟Awesome Tutorials for TensorFlow2<a href="https://github.com/XavierSpycy/Awesome-Tutorials-for-TensorFlow2">&#x2197;</a></h3>
      </summary>
    </details>
</details>

<details>
  <summary>
    <h2>Certification</h2>
  </summary>
  <img src="https://img.shields.io/badge/Coursera-F9AB00?style=for-the-badge&logo=Coursera&color=525252">
  <div align="center">
      <table>
          <tr>
            <td>Specialization</td><td>Launcher</td><td>Completion Date</td><td>Credential</td>
          </tr>
          <tr>
            <td><b>Generative Adversarial Networks (GANs)</b></td>
            <td>DeepLearning.AI</td>
            <td>Jun 2024</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/VMERTE79P6FD">Link</a></td>
          </tr>
          <tr>
            <td><b>Natural Language Processing</b></td>
            <td>DeepLearning.AI</td>
            <td>Oct 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/MGL3LJNX3RZK">Link</a></td>
          </tr>
          <tr>
            <td><b>Deep Learning</b></td>
            <td>DeepLearning.AI</td>
            <td>Aug 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/MCHWQ64ZN86Q">Link</a></td>
          </tr>
          <tr>
            <td><b>Mathematics for Machine Learning and Data Science</b></td>
            <td>DeepLearning.AI</td>
            <td>Aug 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/UJQNSGBXB4FL">Link</a></td>
          </tr>
          <tr>
            <td><b>Applied Data Science with Python</b></td>
            <td>University of Michigan</td>
            <td>Jul 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/S77MYG6WQCUS">Link</a></td>
          </tr>
          <tr>
            <td><b>Machine Learning</b></td>
            <td>DeepLearning.AI & Stanford University</td>
            <td>Jul 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/GVJJK3HZRSLZ">Link</a></td>
          </tr>
          <tr>
            <td><b>Mathematics for Machine Learning</b></td>
            <td>Imperial College London</td>
            <td>Jun 2023</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/UHPTL6AM2WHK">Link</a></td>
          </tr>
          <tr>
            <td><b>Expressway to Data Science: Python Programming</b></td>
            <td>University of Colorado Boulder</td>
            <td>Dec 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/D9N32LL56ZE8">Link</a></td>
          </tr>
          <tr>
            <td><b>Python 3 Programming</b></td>
            <td>University of Michigan</td>
            <td>Dec 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/5ZZC4LJULZ83">Link</a></td>
          </tr>
          <tr>
            <td><b>Introduction to Scripting in Python</b></td>
            <td>Rice University</td>
            <td>Nov 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/KN3NPRDUTAFG">Link</a></td>
          </tr>
          <tr>
            <td><b>Statistics with Python</b></td>
            <td>University of Michigan</td>
            <td>Nov 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/2GDKV3UYSTNU">Link</a></td>
          </tr>
          <tr>
            <td><b>Excel Skills for Data Analytics and Visualization</b></td>
            <td>Macquarie University</td>
            <td>Oct 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/R9Y9AFUZURMK">Link</a></td>
          </tr>
          <tr>
            <td><b>Python for Everybody</b></td>
            <td>University of Michigan</td>
            <td>Oct 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/P2L44FCJR9V6">Link</a></td>
          </tr>
          <tr>
            <td><b>Excel Skills for Business</b></td>
            <td>Macquarie University</td>
            <td>Sep 2022</td>
            <td><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/PKTDSLULS3SB">Link</a></td>
          </tr>
        </tr>
      </table>
  </div>
  <details>
    <summary><h3>Credentials</h3></summary>
    <img src="assets/certificate-wall-2.jpg">
    <img scr="assets/certificate-wall-1.jpg">
  </details>
</details>

<details> 
  <summary>
    <h2>How to Reach me</h2>
  </summary>
  <p align="left">
      <a href="mailto:jixu9182@uni.sydney.edu.au" target="blank">
          <img align="center" src="assets/gmail.svg" alt="Gmail" height="30" width="30" style="margin-right:10px;"/>
          Email: jixu9182@uni.sydney.edu.au
      </a>
  </p>
  <p align="left">
      <a href="https://www.linkedin.com/in/jiarui-xu-xavierspycy98" target="blank">
          <img align="center" src="assets/linkedin.svg" alt="LinkedIn" height="30" width="30" style="margin-right:10px;"/>
          LinkedIn: Jiarui XU
      </a>
  </p>
</details>

## Thank you for visiting :heart: